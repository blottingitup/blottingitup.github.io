---
layout: single
title:  "[Mathematics for ML] Matrix Decomposition"
categories:
  - LGAimers7th
---

# Review on Matrix Decomposition

## Determinant, Invertibility and Trace
The determinant of a square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is a function that maps $\mathbf{A}$ onto a real number.

$$\det(\mathbf{A}) = \left\lvert \mathbf{A} \right\rvert = 
\begin{vmatrix} 
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{vmatrix}.
$$

Let's think first for scalars ($\mathbf{A} = a$), 

$$\mathbf{A}^{-1} = \frac{1}{a}\,(a \neq 0).$$

Then for 2 $\times$ 2 matrices ($\mathbf{A} \in \mathbb{R}^{2 \times 2}$), 

$$\mathbf{A^{-1}} = \frac{1}{a_{11}a_{22} - a_{12}a_{21}} 
\begin{pmatrix} 
a_{22} & -a_{12} \\ 
-a_{21} & a_{11} 
\end{pmatrix}.$$

The formula for 2 $\times$ 2 matrices only holds *iff* 

$$a_{11}a_{22} - a_{12}a_{21} \neq 0.$$

Thus, it can be said $\mathbf{A}$ is invertible *iff* 

$$\det(\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21} \neq 0.$$

We can expand this to n $\times$ n matrices, which can be represented in the following theorem:

**Theorem 1.1.** For any square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ it holds that $\mathbf{A}$ is invertible *iff* $\det(\mathbf{A}) \neq 0.$

How do we obtain $\det(\mathbf{A})$ for $\mathbf{A} \in \mathbb{R}^{n \times n}$ ?

**Theorem 1.2.** (Laplace Expansion). Consider a matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$. Then, for all $j = 1,\dots,n$:  
*1. Expansion along column* $j$

$$\det(\mathbf{A}) = \displaystyle\sum_{k=1}^{n}{(-1)^{k+j}a_{kj}\det(\mathbf{A}_{k,j})}$$

*2. Expansion along row* $j$

$$\det(\mathbf{A}) = \displaystyle\sum_{k=1}^{n}{(-1)^{k+j}a_{jk}\det(\mathbf{A}_{j,k})}$$

where $\mathbf{A}_{k,j} \in \mathbb{R}^{(n-1) \times (n-1)}$ is the *submatrix of* $\mathbf{A}$ that we get deleting row $k$ and column $j$.

**Properties** of the determinant for $\mathbf{A} \in \mathbb{R}^{n \times n}:$
1. $\det(\mathbf{AB}) = \det(\mathbf{A})\det(\mathbf{B}).$
2. $\det(\mathbf{A}) = \det(\mathbf{A}^{\top}).$
3. For a regular $\det(\mathbf{A})$, $\det(\mathbf{A}^{-1}) = \frac{1}{\det(\mathbf{A})}.$
4. For two similar matrices $\mathbf{A}$, $\mathbf{A'}$, $\det(\mathbf{A}) = \det(\mathbf{A'}).$
5. For a triangular matrix $\mathbf{T}$, $\det(\mathbf{T}) = \displaystyle\prod_{i=1}^n T_{ii}.$
6. Adding a multiple of a column/row to another one does not change $\det(\mathbf{A}).$
7. Multiplication of a column/row with $\lambda$ scales $\det(\mathbf{A})$: ($\det(\mathbf{\lambda A})={\lambda^n}\mathbf{A}$).
8. Swapping two rows/columns changes the sign of $\det(\mathbf{A}).$

The last four properties allow using Gaussian elimination to compute det(A) by bringing A into row-echelon form. Gaussian elimination can be stopped when A has reached triangular form.

**Theorem 1.3.** A square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ has $\det(\mathbf{A}) \neq 0$ *iff* rk($\mathbf{A}$) = $n$ (i.e. full rank).

The trace of a square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is defined as  
$$tr(\mathbf{A}) := \displaystyle\sum_{i=1}^{n}a_{ii}$$


**Properties** of the trace for $\mathbf{A} \in \mathbb{R}^{n \times n}:$
1. $tr(\mathbf{A}+\mathbf{B}) = tr(\mathbf{A}) + tr(\mathbf{B})$
2. $tr(\alpha \mathbf{A}) = \alpha tr(\mathbf{A})$
3. $tr(\mathbf{I}_n) = n$


## Eigenvalues and Eigenvectors

$\lambda \in \mathbb{R}$ is an eigenvalue of $\mathbf{A} \in \mathbb{R}^{n \times n}$, and $\mathbf{x} \in \mathbb{R}^n$ is the corresponding eigenvector of $\mathbf{A}$ if $\mathbf{A}\mathbf{x}=\lambda\mathbf{x}.$  
The equivalent states are:
* $\lambda$ is an eigenvalue.
* $(\mathbf{A}-\lambda\mathbf{I}_n)\mathbf{x} = 0$ can be solved non-trivially, i.e. $\mathbf{x} \neq 0.$
* $\det(\mathbf{A}-\lambda\mathbf{I}_n) = 0.$

Eigenvectors are not unique, thus can be represented as a multiplicate of an eigenvector (span).  

For $\mathbf{A} \in \mathbb{R}^{n \times n}$, the set of all eigenvectors of $\mathbf{A}$ associated with an eigenvalue $\lambda$ spans a subspace of $\mathbb{R}^n$, which is called the eigenspace of $\mathbf{A}$ with respect to $\lambda$.  

**Theorem 2.1.** The eigenvectors of a matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ with $n$ distinct eigenvalues are linearly independent. The converse is not true.  

**Theorem 2.2.** Given a matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$, we can always obtain a symmetric, positive semidefinite matrix $\mathbf{S} \in \mathbb{R}^{n \times n}$ by defining

$$\mathbf{S} := \mathbf{A}^{\top}\mathbf{A}.$$

If $rk(\mathbf{A}) = n$, then $\mathbf{S} := \mathbf{A}^{\top}\mathbf{A}$ is symmetric, positive definite.  

**Theorem 2.3.** (Spectral Theorem). If $\mathbf{A} \in \mathbb{R}^{n \times n}$ is symmetric, there exists an orthonormal basis of the corresponding vector space $V$ consisting of eigenvectors of $\mathbf{A}$, and each eigenvalue is real.

The determinant and trace of a matrix of $\mathbf{A} \in \mathbb{R}^{n \times n}$ is each the product and sum of its eigenvalues.  
$$\det(\mathbf{A}) = \displaystyle\prod_{i=1}^n \lambda_i, tr(\mathbf{A}) = \displaystyle\sum_{i=1}^n \lambda_i.$$


### Cholesky Decomposition
**Theorem 3.1.** (Cholesky Decomposition). A symmetric, positive definite matrix $\mathbf{A}$ can be factorized into a product $\mathbf{A=LL}^\top$, where $\mathbf{L}$ is a lower-triangular matrix with positive diagonal elements. $\mathbf{L}$ is called the Cholesky factor of $\mathbf{A}$, and $\mathbf{L}$ is unique.
Applications of Cholesky decomposition can be:
1. Factorization of covariance matrix of a multivariate Guassian variable.
2. Linear transformation of random variables.
3. Fast determinant computation: $\det(\mathbf{A}) = \det(\mathbf{L})\det(\mathbf{L}^\top) = (\det(\mathbf{L}))^2 = \displaystyle\prod_{i=1}^n \mathbf{I}_{ii}^2.$


### Eigendecomposition and Diagonalization
A diagonal matrix is a matrix that has value zero on all off-diagonal elements, taking the form: 

$$\mathbf{D} = 
\begin{pmatrix} 
d_1 & \cdots & 0 \\ 
\vdots & & \vdots \\
0 & \cdots & d_n
\end{pmatrix}.$$

$\mathbf{A} \in \mathbb{R}^{n \times n}$ is diagonalizable if it is similar to a diagonal matrix $\mathbf{D}$, i.e., there exixts an invertible $\mathbf{P} \in \mathbb{R}^{n \times n}$, such that $\mathbf{D}=\mathbf{P}^{-1}\mathbf{AP}.$  
$\mathbf{A} \in \mathbb{R}^{n \times n}$ is orthogonally diagonalizable if it is similar to a diagonal matrix $\mathbf{D}$, i.e., there exixts an orthogonal $\mathbf{P} \in \mathbb{R}^{n \times n}$, such that $\mathbf{D}=\mathbf{P}^{-1}\mathbf{AP}=\mathbf{P}^{\top}\mathbf{AP}.$  
**Theorem 4.1.** (Eigendecomposition). A square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ can be factored into

$$\mathbf{A}=\mathbf{PDP}^{-1},$$  

where $\mathbf{P} \in \mathbb{R}^{n \times n}$ and $\mathbf{D}$ is a diagonal matrix whose diagonal entries are the eigenvalues of $\mathbf{A}$, *iff* the eigenvectors of $\mathbf{A}$ form a basis of $\mathbb{R}^n.$

**Theorem 4.2.** A symmetric matrix $\mathbf{S} \in \mathbb{R}^{n \times n}$ can always be diagonalized.  
This follows from the spectral theorem(2.3), implying that the columns of $\mathbf{P}$ are the $n$ eigenvectors of $\mathbf{A}$, and the diagonal elements of $\mathbf{D}$ are the corresponding eigenvalues of $\mathbf{A}.$
If the eigendecomposition exists for a matrix $\mathbf{P} \in \mathbb{R}^{n \times n}$, the following can be done:
1. $\mathbf{A}^k = (\mathbf{PDP}^{-1})^k = \mathbf{PD}^k\mathbf{P}^{-1}.$
2. $\det(\mathbf{A}) = \det(\mathbf{PDP}^{-1}) = \det(\mathbf{P})\det(\mathbf{D})\det(\mathbf{P}^{-1}) = \det(\mathbf{D}) = \displaystyle\prod_{i=1}^n d_{ii}.$


### Singular Value Decomposition
**Theorem 5.1.** (SVD Theorem). Let $\mathbf{A} \in \mathbb{R}^{m \times n}$ be a rectangular matrix of rank $r \in [0, min(m, n)].$ The SVD of $\mathbf{A}$ is a decomposition of the form $\mathbf{A} = \mathbf{U}\Sigma\mathbf{V}^{\top}.$  
* The orthogonal matrix $\mathbf{U} \in \mathbb{R}^{m \times m}$ has column vectors (left singular vectors) $\mathbf{u}_i (i = 1, \dots, m).$
* The orthogonal matrix $\mathbf{V} \in \mathbb{R}^{n \times n}$ has column vectors (right singular vectors) $\mathbf{v}_i (i = 1, \dots, n).$
* The singular value matrix $\Sigma \in \mathbb{R}^{m \times n}$ is a matrix with $\Sigma_{ii} = \sigma \geq 0, \Sigma_{ij} = 0, i \neq j$, where the diagonal entries $\sigma_i (i = 1, \dots, r)$ are called singular values.
