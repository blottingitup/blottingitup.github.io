---
layout: single
title:  "[Mathematics for ML] Matrix Decomposition"
categories:
  - LGAimers7th
---

# Review on Matrix Decomposition

### Determinant, Invertibility and Trace
The determinant of a square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is a function that maps $\mathbf{A}$ onto a real number.

$$\det(\mathbf{A}) = \left\lvert \mathbf{A} \right\rvert = 
\begin{vmatrix} 
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{vmatrix}.
$$

Let's think first for scalars ($\mathbf{A} = a$), 

$$\mathbf{A}^{-1} = \frac{1}{a}\,(a \neq 0).$$

Then for 2 $\times$ 2 matrices ($\mathbf{A} \in \mathbb{R}^{2 \times 2}$), 

$$\mathbf{A^{-1}} = \frac{1}{a_{11}a_{22} - a_{12}a_{21}} 
\begin{pmatrix} 
a_{22} & -a_{12} \\ 
-a_{21} & a_{11} 
\end{pmatrix}.$$

The formula for 2 $\times$ 2 matrices only holds *iff* 

$$a_{11}a_{22} - a_{12}a_{21} \neq 0.$$

Thus, it can be said $\mathbf{A}$ is invertible *iff* 

$$\det(\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21} \neq 0.$$

We can expand this to n $\times$ n matrices, which can be represented in the following theorem:

**Theorem 1.1.** For any square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ it holds that $\mathbf{A}$ is invertible *iff* $\det(\mathbf{A}) \neq 0.$

How do we obtain $\det(\mathbf{A})$ for $\mathbf{A} \in \mathbb{R}^{n \times n}$ ?

**Theorem 1.2.** (Laplace Expansion). Consider a matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$. Then, for all $j = 1,\dots,n$:  
*1. Expansion along column* $j$

$$\det(\mathbf{A}) = \displaystyle\sum_{k=1}^{n}{(-1)^{k+j}a_{kj}\det(\mathbf{A}_{k,j})}$$

*2. Expansion along row* $j$

$$\det(\mathbf{A}) = \displaystyle\sum_{k=1}^{n}{(-1)^{k+j}a_{jk}\det(\mathbf{A}_{j,k})}$$

where $\mathbf{A}_{k,j} \in \mathbb{R}^{(n-1) \times (n-1)}$ is the *submatrix of* $\mathbf{A}$ that we get deleting row $k$ and column $j$.

(show 3x3 matrix diff: Sarrus's rule)

**Properties** of the determinant for $\mathbf{A} \in \mathbb{R}^{n \times n}:$
1. $\det(\mathbf{AB}) = \det(\mathbf{A})\det(\mathbf{B}).$
2. $\det(\mathbf{A}) = \det(\mathbf{A}^{\top}).$
3. For a regular $\det(\mathbf{A})$, $\det(\mathbf{A}^{-1}) = \frac{1}{\det(\mathbf{A})}.$
4. For two similar matrices $\mathbf{A}$, $\mathbf{A'}$, $\det(\mathbf{A}) = \det(\mathbf{A'}).$
5. For a triangular matrix $\mathbf{T}$, $\det(\mathbf{T}) = \displaystyle\prod_{i=1}^n T_{ii}.$
6. Adding a multiple of a column/row to another one does not change $\det(\mathbf{A}).$
7. Multiplication of a column/row with $\lambda$ scales $\det(\mathbf{A})$: ($\det(\mathbf{\lambda A})={\lambda^n}\mathbf{A}$).
8. Swapping two rows/columns changes the sign of $\det(\mathbf{A}).$

The last four properties allow using Gaussian elimination to compute det(A) by bringing A into row-echelon form. Gaussian elimination can be stopped when A has reached triangular form.

**Theorem 1.3.** A square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ has $\det(\mathbf{A}) \neq 0$ *iff* rk($\mathbf{A}$) = $n$ (i.e. full rank).

The trace of a square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is defined as  
$$tr(\mathbf{A}) := \displaystyle\sum_{i=1}^{n}a_{ii}$$


**Properties** of the trace for $\mathbf{A} \in \mathbb{R}^{n \times n}:$
1. $tr(\mathbf{A}+\mathbf{B}) = tr(\mathbf{A}) + tr(\mathbf{B})$
2. $tr(\alpha \mathbf{A}) = \alpha tr(\mathbf{A})$
3. $tr(\mathbf{I}_n) = n$


### Eigenvalues and Eigenvectors

$\lambda \in \mathbb{R}$ is an eigenvalue of $\mathbf{A} \in \mathbb{R}^{n \times n}$, and $\mathbf{x} \in \mathbb{R}^n  $ is the corresponding eigenvector of $\mathbf{A}$ if $\mathbf{A}\mathbf{x}=\lambda\mathbf{x}.$  
The equivalent states are:
* $\lambda$ is an eigenvalue.
* $(\mathbf{A}-\lambda\mathbf{I}_n)\mathbf{x} = 0$ can be solved non-trivially, i.e. $\mathbf{x} \neq 0.$
* $\det(\mathbf{A}-\lambda\mathbf{I}_n) = 0.$

Eigenvectors are not unique, thus can be represented as a multiplicate of an eigenvector (span).  

For $\mathbf{A} \in \mathbb{R}^{n \times n}$, the set of all eigenvectors of $\mathbf{A}$ associated with an eigenvalue $\lambda$ spans a subspace of $\mathbb{R}^n$, which is called the eigenspace of $\mathbf{A}$ with respect to $\lambda$.  

**Theorem 1.4.** The eigenvectors of a matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ with $n$ distinct eigenvalues are linearly independent. The converse is not true.  

**Theorem 1.5.** Given a matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$, we can always obtain a symmetric, positive semidefinite matrix $\mathbf{S} \in \mathbb{R}^{n \times n}$ by defining

$$\mathbf{S} := \mathbf{A}^{\top}\mathbf{A}.$$

If $rk(\mathbf{A}) = n$, then $\mathbf{S} := \mathbf{A}^{\top}\mathbf{A}$ is symmetric, positive definite.  

**Theorem 1.6.** (Spectral Theorem). If $\mathbf{A} \in \mathbb{R}^{n \times n}$ is symmetric, there exists an orthonormal basis of the corresponding vector space $V$ consisting of eigenvectors of $\mathbf{A}$, and each eigenvalue is real.

The determinant and trace of a matrix of $\mathbf{A} \in \mathbb{R}^{n \times n}$ is each the product and sum of its eigenvalues.  
$$\det(\mathbf{A}) = \displaystyle\prod_{i=1}^n \lambda_i, tr(\mathbf{A}) = \displaystyle\sum_{i=1}^n \lambda_i.$$


### Cholesky Decomposition
**Theorem 1.7.** (Cholesky Decomposition). 


### Eigendecomposition and Diagonalization
d


### Singular Value Decomposition
s

