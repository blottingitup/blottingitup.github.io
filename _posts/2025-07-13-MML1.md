---
layout: single
title:  "[Mathematics for ML] Matrix Decomposition"
categories:
  - LGAimers7th
---

# Review on Matrix Decomposition

### Determinant, Invertibility and Trace
The determinant of a square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is a function that maps $\mathbf{A}$ onto a real number.

$$\det(\mathbf{A}) = \left\lvert \mathbf{A} \right\rvert = 
\begin{vmatrix} 
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots  & \vdots  & \ddots & \vdots  \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{vmatrix}.
$$

Let's think first for scalars ($\mathbf{A} = a$), 

$$\mathbf{A}^{-1} = \frac{1}{a}\,(a \neq 0).$$

Then for 2 $\times$ 2 matrices ($\mathbf{A} \in \mathbb{R}^{2 \times 2}$), 

$$\mathbf{A^{-1}} = \frac{1}{a_{11}a_{22} - a_{12}a_{21}} 
\begin{pmatrix} 
a_{22} & -a_{12} \\ 
-a_{21} & a_{11} 
\end{pmatrix}.$$

The formula for 2 $\times$ 2 matrices only holds *iff* 

$$a_{11}a_{22} - a_{12}a_{21} \neq 0.$$

Thus, it can be said $\mathbf{A}$ is invertible *iff* 

$$\det(\mathbf{A}) = a_{11}a_{22} - a_{12}a_{21} \neq 0.$$

We can expand this to n $\times$ n matrices, which can be represented in the following theorem:

**Theorem 1.1.** For any square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ it holds that $\mathbf{A}$ is invertible *iff* $\det(\mathbf{A}) \neq 0.$

How do we obtain $\det(\mathbf{A})$ for $\mathbf{A} \in \mathbb{R}^{n \times n}$ ?

**Theorem 1.2.** (Laplace Expansion). Consider a matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$. Then, for all $j = 1,\dots,n$:  
*1. Expansion along column* $j$

$$\det(\mathbf{A}) = \displaystyle\sum_{k=1}^{n}{(-1)^{k+j}a_{kj}\det(\mathbf{A}_{k,j})}$$

*2. Expansion along row* $j$

$$\det(\mathbf{A}) = \displaystyle\sum_{k=1}^{n}{(-1)^{k+j}a_{jk}\det(\mathbf{A}_{j,k})}$$

where $\mathbf{A}_{k,j} \in \mathbb{R}^{(n-1) \times (n-1)}$ is the *submatrix of* $\mathbf{A}$ that we get deleting row $k$ and column $j$.

(show 3x3 matrix diff: Sarrus's rule)

**Properties** of the determinant for $\mathbf{A} \in \mathbb{R}^{n \times n}:$
1. $\det(\mathbf{AB}) = \det(\mathbf{A})\det(\mathbf{B}).$
2. $\det(\mathbf{A}) = \det(\mathbf{A}^\mathbf{T}).$
3. For a regular $\det(\mathbf{A})$, $\det(\mathbf{A}^{-1}) = \frac{1}{\det(\mathbf{A})}.$
4. For two similar matrices $\mathbf{A}$, $\mathbf{A'}$, $\det(\mathbf{A}) = \det(\mathbf{A'}).$
5. For a triangular matrix $\mathbf{T}$, $\det(\mathbf{T}) = \displaystyle\prod_{i=1}^n T_{ii}.$
6. Adding a multiple of a column/row to another one does not change $\det(\mathbf{A}).$
7. Multiplication of a column/row with $\lambda$ scales $\det(\mathbf{A})$: ($\det(\mathbf{\lambda A})={\lambda^n}\mathbf{A}$).
8. Swapping two rows/columns changes the sign of $\det(\mathbf{A}).$

The last four properties allow using Gaussian elimination to compute det(A) by bringing A into row-echelon form. Gaussian elimination can be stopped when A has reached triangular form.

**Theorem 1.3.** A square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ has $\det(\mathbf{A}) \neq 0$ *iff* rk($\mathbf{A}$) = $n$ (i.e. full rank).

The trace of a square matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is defined as  
tr($\mathbf{A}$) := $\displaystyle\sum_{i=1}^{n}a_{ii}$


**Properties** of the trace for $\mathbf{A} \in \mathbb{R}^{n \times n}:$
1. tr(A+B)=tr(A)+tr(B)
2. tr(aA)=atr(A)
3. tr(I_n)=n


### Eigenvalues and Eigenvectors

$\lambda \in \mathbb{R}$ is an eigenvalue of $\mathbf{A} \in \mathbb{R}^{n \times n}$ and $\mathbf{x} \in \mathbb{R}^n \\ \{0\}$ is the corresponding eigenvector of $\mathbf{A}$ if $\mathbf{A}\mathbf{x}=\lambda\mathbf{x}$
equivalent statements
eigenvectors are not unique
theorem:eigenvector independent inverse not true
spectral theorem
determinant
trace


### Cholesky Decomposition
d


### Eigendecomposition and Diagonalization
d


### Singular Value Decomposition
s

